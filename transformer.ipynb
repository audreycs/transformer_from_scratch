{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center;\">\n",
    "#     <img src=\"./transformer.webp\" alt=\"Alt text\" width=\"500\">\n",
    "# </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Encoder Block\n",
    "\n",
    "Basically it consists of:\n",
    "- Input Embeddings\n",
    "- Multi-head Self-attention Layer\n",
    "- Feedforward Network (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Input Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Create input sequence and input embeddings\n",
    "Create a vocabulary embedding table and look for embedding of each token. **This embedding table is trainable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_seq = ['<sos>', 'hello', 'world', '<eos>']\n",
    "input_tokens = [0, 87, 101, 999] # just for example, in practice, there is a tokenizer to convert the input sequence to tokens\n",
    "\n",
    "# imagine we have a vocabulary of 1000 words\n",
    "vocab_size = 1000\n",
    "embedding_dim = 64\n",
    "\n",
    "vocab_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# create input embeddings\n",
    "input_embeddings = vocab_embedding_table(torch.LongTensor(input_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Create positional encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "max_seq_len = len(input_seq)\n",
    "positional_encodings = torch.zeros(max_seq_len, embedding_dim)\n",
    "\n",
    "for i in range(max_seq_len):\n",
    "    for j in range(embedding_dim):\n",
    "        if j % 2 == 0:\n",
    "            positional_encodings[i, j] = math.sin(i / (10000 ** (j / embedding_dim)))\n",
    "        else:\n",
    "            positional_encodings[i, j] = math.cos(i / (10000 ** ((j - 1) / embedding_dim)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Add input embeddings and positional encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = input_embeddings + positional_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Multi-Head Self-Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, K, V = input_embeddings, input_embeddings, input_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Create query, key, value matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64]) torch.Size([4, 64]) torch.Size([4, 64])\n"
     ]
    }
   ],
   "source": [
    "W_q = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "W_k = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "W_v = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "\n",
    "Q = W_q(input_embeddings)\n",
    "K = W_k(input_embeddings)\n",
    "V = W_v(input_embeddings)\n",
    "\n",
    "print(Q.shape, K.shape, V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Convert Shapes As Multi-Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 32]) torch.Size([2, 4, 32]) torch.Size([2, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "head_num = 2 # for example\n",
    "\n",
    "Q = Q.view(head_num, len(input_seq), embedding_dim//head_num)\n",
    "K = K.view(head_num, len(input_seq), embedding_dim//head_num)\n",
    "V = V.view(head_num, len(input_seq), embedding_dim//head_num)\n",
    "\n",
    "print(Q.shape, K.shape, V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Calculate Attentions\n",
    "This include **matrix multiplication**, **scaling**, and **softmax**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# matrix multiplication\n",
    "QK = torch.matmul(Q, K.transpose(1, 2))\n",
    "\n",
    "# scaling\n",
    "QK = QK / math.sqrt(embedding_dim//head_num)\n",
    "\n",
    "# softmax\n",
    "QK = torch.softmax(QK, dim=-1)\n",
    "\n",
    "print(QK.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Multiply Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "QKV = torch.matmul(QK, V)\n",
    "\n",
    "print(QKV.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Concatenating the Heads & Linear Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_head_output = QKV.transpose(0, 1).contiguous()\n",
    "multi_head_output = multi_head_output.view(len(input_seq), embedding_dim)\n",
    "\n",
    "# linear projection\n",
    "W_o = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "multi_head_output = W_o(multi_head_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Add & Norm\n",
    "- Add residual connection\n",
    "- Layernorm\n",
    "- Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual connection\n",
    "output = input_embeddings + multi_head_output\n",
    "\n",
    "# layer normalization\n",
    "output = nn.LayerNorm(embedding_dim)(output)\n",
    "output = nn.Dropout(0.3)(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 FeedForward Network (MLP)\n",
    "**Two Linear layers.** Also in the end, it has layernorm and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_ff_1 = nn.Linear(embedding_dim, 4*embedding_dim)\n",
    "W_ff_2 = nn.Linear(4*embedding_dim, embedding_dim)\n",
    "\n",
    "# feedforward: linear -> relu -> linear\n",
    "ff_output = W_ff_1(output)\n",
    "ff_output = nn.ReLU()(ff_output)\n",
    "ff_output = W_ff_2(ff_output)\n",
    "\n",
    "# residual connection\n",
    "ff_output = output + ff_output\n",
    "\n",
    "# layer normalization\n",
    "ff_output = nn.LayerNorm(embedding_dim)(ff_output)\n",
    "ff_output = nn.Dropout(0.3)(ff_output)\n",
    "\n",
    "encoder_output = ff_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Decoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Output Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Create output embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_seq = ['<sos>', 'I', 'am', 'here', '<eos>']\n",
    "output_tokens = [0, 101, 102, 103, 999] # just for example, in practice, there is a tokenizer to convert the input sequence to tokens\n",
    "\n",
    "output_embeddings = vocab_embedding_table(torch.LongTensor(output_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Create positional encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_encodings = torch.zeros(len(output_seq), embedding_dim)\n",
    "\n",
    "for i in range(len(output_seq)):\n",
    "    for j in range(embedding_dim):\n",
    "        if j % 2 == 0:\n",
    "            positional_encodings[i, j] = math.sin(i / (10000 ** (j / embedding_dim)))\n",
    "        else:\n",
    "            positional_encodings[i, j] = math.cos(i / (10000 ** ((j - 1) / embedding_dim)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Add output embeddings and positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 64])\n"
     ]
    }
   ],
   "source": [
    "output_embeddings = output_embeddings + positional_encodings\n",
    "\n",
    "print(output_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Multi-Head Self-Attention and Cross-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Masked Self-Attention Layer\n",
    "Just do multi-head self attention on output tokens. **Note that now there is a mask during QK multiplication**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 64])\n"
     ]
    }
   ],
   "source": [
    "W_q = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "W_k = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "W_v = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "\n",
    "Q = W_q(output_embeddings)\n",
    "K = W_k(output_embeddings)\n",
    "V = W_v(output_embeddings)\n",
    "\n",
    "Q = Q.view(head_num, len(output_seq), embedding_dim//head_num)\n",
    "K = K.view(head_num, len(output_seq), embedding_dim//head_num)\n",
    "V = V.view(head_num, len(output_seq), embedding_dim//head_num)\n",
    "\n",
    "QK = torch.matmul(Q, K.transpose(1, 2))\n",
    "\n",
    "# generate mask\n",
    "mask = torch.tril(torch.ones(len(output_seq), len(output_seq))).unsqueeze(0)\n",
    "# apply mask\n",
    "QK = QK.masked_fill(mask == 0, -float('inf'))\n",
    "\n",
    "QK = QK / math.sqrt(embedding_dim//head_num)\n",
    "\n",
    "QK = torch.softmax(QK, dim=-1)\n",
    "\n",
    "QKV = torch.matmul(QK, V)\n",
    "\n",
    "## transpose and reshape\n",
    "QKV = QKV.transpose(0, 1).contiguous()\n",
    "QKV = QKV.view(len(output_seq), embedding_dim)\n",
    "\n",
    "# linear projection\n",
    "W_o = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "QKV = W_o(QKV)\n",
    "\n",
    "## add residual connection\n",
    "QKV = output_embeddings + QKV\n",
    "\n",
    "## layer normalization\n",
    "QKV = nn.LayerNorm(embedding_dim)(QKV)\n",
    "QKV = nn.Dropout(0.3)(QKV)\n",
    "\n",
    "self_attention_output = QKV\n",
    "\n",
    "print(self_attention_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Cross-Attention\n",
    "\n",
    "Now, the output of the first attention layer will be only the queries of the second attention layer. The output of encoder block will be the keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 64]) torch.Size([4, 64]) torch.Size([4, 64])\n"
     ]
    }
   ],
   "source": [
    "Q = self_attention_output\n",
    "K = encoder_output\n",
    "V = encoder_output\n",
    "\n",
    "print(Q.shape, K.shape, V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue the multi-head cross-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 32]) torch.Size([2, 4, 32]) torch.Size([2, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "W_q = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "W_k = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "W_v = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "\n",
    "Q = W_q(Q)\n",
    "K = W_k(K)\n",
    "V = W_v(V)\n",
    "\n",
    "Q = Q.view(head_num, len(output_seq), embedding_dim//head_num)\n",
    "K = K.view(head_num, len(input_seq), embedding_dim//head_num)\n",
    "V = V.view(head_num, len(input_seq), embedding_dim//head_num)\n",
    "\n",
    "print(Q.shape, K.shape, V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 4])\n",
      "torch.Size([2, 5, 32])\n"
     ]
    }
   ],
   "source": [
    "QK = torch.matmul(Q, K.transpose(1, 2))\n",
    "\n",
    "QK = QK / math.sqrt(embedding_dim//head_num)\n",
    "QK = torch.softmax(QK, dim=-1)\n",
    "\n",
    "print(QK.shape)\n",
    "\n",
    "QKV = torch.matmul(QK, V)\n",
    "\n",
    "print(QKV.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating Heads & Linear Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "QKV = QKV.transpose(0, 1).contiguous()\n",
    "QKV = QKV.view(len(output_seq), embedding_dim)\n",
    "\n",
    "# linear projection\n",
    "W_o = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "QKV = W_o(QKV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add residual connection\n",
    "QKV = self_attention_output + QKV\n",
    "\n",
    "# layer normalization\n",
    "QKV = nn.LayerNorm(embedding_dim)(QKV)\n",
    "QKV = nn.Dropout(0.3)(QKV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Feedforward Layer (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 64])\n"
     ]
    }
   ],
   "source": [
    "W_ff_1 = nn.Linear(embedding_dim, 4*embedding_dim)\n",
    "W_ff_2 = nn.Linear(4*embedding_dim, embedding_dim)\n",
    "\n",
    "# feedforward: linear -> relu -> linear\n",
    "output = W_ff_1(QKV)\n",
    "output = nn.ReLU()(output)\n",
    "output = W_ff_2(output)\n",
    "\n",
    "# add residual connection\n",
    "output = QKV + output\n",
    "\n",
    "# layer normalization\n",
    "output = nn.LayerNorm(embedding_dim)(output)\n",
    "output = nn.Dropout(0.3)(output)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Decoding Layer\n",
    "A final linear layer and a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(952)\n"
     ]
    }
   ],
   "source": [
    "W_dec = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "logits = W_dec(output)\n",
    "\n",
    "logits = nn.Softmax(dim=-1)(logits)\n",
    "\n",
    "# get the logits at the last position\n",
    "logits = logits[-1, :]\n",
    "\n",
    "# get next token\n",
    "next_token = torch.argmax(logits, dim=-1)\n",
    "\n",
    "print(next_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
